\section{Overview of Machine Learning Algorithms}
\subsection{Supervised Sequence Labelling with Recurrent Neural Networks \cite{Graves}}
Artificial neural networks (ANNs) were originally developed as mathematical models of the information processing capabilities of biological brains. Although it is now clear that ANNs bear little resemblance to real biological neurons, they enjoy continuing popularity as pattern classifiers. The basic structure of an ANN is a network of small processing units, or nodes, joined to each other by weighted connections. In terms of the original biological model, the nodes represent neurons, and the connection weights represent the strength of the synapses between the neurons. The network is activated by providing an input to some or all of the nodes, and this activation then spreads throughout the network along the weighted connections. ANNs without cycles are referred to as feedforward neural networks (FNNs). Well known examples of FNNs include perceptrons, radial basis function networks, Kohonen maps and Hopfield nets. The most widely used form of FNN is the multilayer perceptron(MLP). It has been proven that an MLP with a single hidden layer containing a sufficient number of nonlinear units can approximate any continuous function on a compact input domain to arbitrary precision. For this reason MLPs are said to be universal function approximators.
At each unit in a layer the activation function $\theta_h$ is applied, yielding the final activation $b_h$ of the unit. The most common choices for neural network activation function are the hyperbolic tangent and the logistic sigmoid. An important feature of both tanh and the logistic sigmoid is their nonlinearity. Nonlinear neural networks are more powerful than linear ones since they can, for example, find nonlinear classification boundaries and model nonlinear equations. Another key property is that both functions are differentiable, which allows the network to be trained with gradient descent. Because of the way they reduce an infinite input domain to a finite output range, neural network activation functions are sometimes referred to as squashing functions. The output vector $y$ of an MLP is given by the activation of the units in the output layer. The network input $a_k$ to each output unit $k$ is calculated by summing over the units connected to it, exactly as for a hidden unit. Both the number of units in the output layer and the choice of output activation function depend on the task the network is applied to. For classification problems with $K > 2$ classes, the convention is to have $K$ output units, and normalise the output activations with the softmax function to obtain the class probabilities which is also known as a multinomial logit model.
