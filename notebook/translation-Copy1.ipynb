{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.datasets import mnist as mn\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.01\n",
    "num_steps = 500\n",
    "batch_size = 128\n",
    "n_images = 10000\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.25 # Dropout, probability to drop a unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the neural network\n",
    "def conv_net(x_dict, n_classes, dropout, reuse, is_training):\n",
    "    \n",
    "    # Define a scope for reusing the variables\n",
    "    with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "        # TF Estimator input is a dict, in case of multiple inputs\n",
    "        x = x_dict['images']\n",
    "\n",
    "        # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "        # Reshape to match picture format [Height x Width x Channel]\n",
    "        # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "        x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "        # Convolution Layer with 32 filters \n",
    "        conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "\n",
    "        # Convolution Layer with 64 filters \n",
    "        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
    "\n",
    "        # Flatten the data to a 1-D vector for the fully connected layer\n",
    "        fc1 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "        # Fully connected layer (in tf contrib folder for now)\n",
    "        fc1 = tf.layers.dense(fc1, 1024)\n",
    "        # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "\n",
    "        # Output layer, class prediction\n",
    "        out = tf.layers.dense(fc1, n_classes)\n",
    "\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model function (following TF Estimator Template)\n",
    "def model_fn(features, labels, mode):\n",
    "    \n",
    "    # Build the neural network\n",
    "    # Because Dropout have different behavior at training and prediction time, we\n",
    "    # need to create 2 distinct computation graphs that still share the same weights.\n",
    "    logits_train = conv_net(features, num_classes, dropout, reuse=False, is_training=True)\n",
    "    logits_test = conv_net(features, num_classes, dropout, reuse=True, is_training=False)\n",
    "    \n",
    "    # Predictions\n",
    "    pred_classes = tf.argmax(logits_test, axis=1)\n",
    "    pred_probas = tf.nn.softmax(logits_test)\n",
    "    \n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes) \n",
    "        \n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    # Evaluate the accuracy of the model\n",
    "    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "    \n",
    "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "    # the different ops for training, evaluating, ...\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=pred_classes,\n",
    "      loss=loss_op,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "    return estim_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/33/cbwqx9c11lgfbcvwxr2yf6jm0000gn/T/tmpvru1_w3e\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/33/cbwqx9c11lgfbcvwxr2yf6jm0000gn/T/tmpvru1_w3e', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1c47cf3240>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# Build the Estimator\n",
    "model = tf.estimator.Estimator(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Could not find trained model in model_dir: /var/folders/33/cbwqx9c11lgfbcvwxr2yf6jm0000gn/T/tmpvru1_w3e, running initialization to evaluate.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-10-10-12:08:05\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-10-10-12:08:14\n",
      "INFO:tensorflow:Saving dict for global step 0: accuracy = 0.0943, global_step = 0, loss = 2.3076017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.0943, 'loss': 2.3076017, 'global_step': 0}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the Model\n",
    "# Define the input function for evaluating\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': mnist.test.images}, y=mnist.test.labels,\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "# Use the Estimator 'evaluate' method\n",
    "model.evaluate(input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Could not find trained model in model_dir: /var/folders/33/cbwqx9c11lgfbcvwxr2yf6jm0000gn/T/tmpvru1_w3e, running initialization to predict.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Model accuracy: 0.12400000000000252\n",
      "INFO:tensorflow:Could not find trained model in model_dir: /var/folders/33/cbwqx9c11lgfbcvwxr2yf6jm0000gn/T/tmpvru1_w3e, running initialization to predict.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Model accuracy: 0.10480000000000197\n",
      "INFO:tensorflow:Could not find trained model in model_dir: /var/folders/33/cbwqx9c11lgfbcvwxr2yf6jm0000gn/T/tmpvru1_w3e, running initialization to predict.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Model accuracy: 0.07180000000000103\n",
      "INFO:tensorflow:Could not find trained model in model_dir: /var/folders/33/cbwqx9c11lgfbcvwxr2yf6jm0000gn/T/tmpvru1_w3e, running initialization to predict.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Model accuracy: 0.105600000000002\n",
      "INFO:tensorflow:Could not find trained model in model_dir: /var/folders/33/cbwqx9c11lgfbcvwxr2yf6jm0000gn/T/tmpvru1_w3e, running initialization to predict.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Model accuracy: 0.052100000000000465\n",
      "Transformation completed!\n"
     ]
    }
   ],
   "source": [
    "f = open(\"shift.txt\", 'w')\n",
    "(X_train, y_train), (X_test, y_test) = mn.load_data()\n",
    "X_test = X_test.astype('float32')\n",
    "n_images = 10000\n",
    "datagen = ImageDataGenerator()# fit parameters from data\n",
    "val = 0.0\n",
    "while val<1:\n",
    "    transform_parameters = { 'tx': val}\n",
    "    X = datagen.apply_transform(X_test, transform_parameters)\n",
    "    x_te={'images': X}\n",
    "    input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x=x_te, shuffle=False)\n",
    "    # Use the model to predict the images class\n",
    "    preds = list(model.predict(input_fn))\n",
    "    accuracy = 0.0\n",
    "    # Display\n",
    "    for i in range(n_images):\n",
    "        if (preds[i] == y_test[i]):\n",
    "            accuracy = accuracy + 1/n_images\n",
    "#         plt.imshow(np.reshape(X[i], [28, 28]), cmap='gray')\n",
    "#         plt.show()\n",
    "#         print(\"Model prediction:\", preds[i])\n",
    "#         print(\"Model val:\", y_test[i])\n",
    "    print(\"Model accuracy:\", accuracy)\n",
    "    f.write(str(val)+\"\\t\"+str(accuracy)+\"\\n\")\n",
    "    val = val+0.2\n",
    "\n",
    "\n",
    "f.close()\n",
    "print(\"Transformation completed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.  84. 185. 159. 151.  60.  36.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0. 222. 254. 254. 254. 254. 241. 198. 198.\n",
      "  198. 198. 198. 198. 198. 198. 170.  52.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.  67. 114.  72. 114. 163. 227. 254. 225.\n",
      "  254. 254. 254. 250. 229. 254. 254. 140.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  17.  66.  14.\n",
      "   67.  67.  67.  59.  21. 236. 254. 106.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.  83. 253. 209.  18.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.  22. 233. 255.  83.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0. 129. 254. 238.  44.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.  59. 249. 254.  62.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0. 133. 254. 187.   5.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   9. 205. 248.  58.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 126. 254. 182.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   75. 251. 240.  57.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  19.\n",
      "  221. 254. 166.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   3. 203.\n",
      "  254. 219.  35.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  38. 254.\n",
      "  254.  77.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  31. 224. 254.\n",
      "  115.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 133. 254. 254.\n",
      "   52.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  61. 242. 254. 254.\n",
      "   52.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 121. 254. 254. 219.\n",
      "   40.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 121. 254. 207.  18.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]\n",
      "[[0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 6.720e+01\n",
      "  1.480e+02 1.272e+02 1.208e+02 4.800e+01 2.880e+01 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.944e+02\n",
      "  2.402e+02 2.350e+02 2.334e+02 2.152e+02 2.000e+02 1.584e+02 1.584e+02\n",
      "  1.584e+02 1.584e+02 1.584e+02 1.584e+02 1.584e+02 1.584e+02 1.360e+02\n",
      "  4.160e+01 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 9.800e+01\n",
      "  1.420e+02 1.084e+02 1.420e+02 1.812e+02 2.298e+02 2.428e+02 2.196e+02\n",
      "  2.428e+02 2.428e+02 2.428e+02 2.396e+02 2.228e+02 2.428e+02 2.372e+02\n",
      "  1.224e+02 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.340e+01\n",
      "  2.280e+01 1.440e+01 2.280e+01 3.260e+01 5.900e+01 1.036e+02 5.620e+01\n",
      "  1.044e+02 1.044e+02 1.044e+02 9.720e+01 6.260e+01 2.396e+02 2.540e+02\n",
      "  1.128e+02 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 3.400e+00 1.320e+01 2.800e+00\n",
      "  1.340e+01 1.340e+01 1.340e+01 1.180e+01 7.060e+01 2.496e+02 2.180e+02\n",
      "  3.560e+01 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.760e+01 2.030e+02 2.546e+02 1.082e+02\n",
      "  3.600e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.076e+02 2.498e+02 2.414e+02 5.180e+01\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 4.720e+01 2.250e+02 2.540e+02 9.720e+01 8.800e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 1.182e+02 2.530e+02 2.004e+02 1.640e+01 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 7.200e+00 1.906e+02 2.492e+02 8.380e+01 1.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 1.026e+02 2.442e+02 1.952e+02 1.160e+01 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  6.000e+01 2.260e+02 2.428e+02 8.200e+01 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.520e+01\n",
      "  1.918e+02 2.534e+02 1.808e+02 1.140e+01 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 2.400e+00 1.662e+02\n",
      "  2.474e+02 2.260e+02 6.120e+01 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 3.100e+01 2.438e+02\n",
      "  2.540e+02 1.054e+02 7.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 2.480e+01 1.868e+02 2.540e+02\n",
      "  1.428e+02 1.620e+01 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.126e+02 2.480e+02 2.540e+02\n",
      "  6.460e+01 2.000e-01 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 4.880e+01 2.202e+02 2.540e+02 2.540e+02\n",
      "  5.200e+01 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.090e+02 2.516e+02 2.540e+02 2.260e+02\n",
      "  4.240e+01 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.210e+02 2.540e+02 2.164e+02 5.820e+01\n",
      "  8.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 2.420e+01 5.080e+01 4.140e+01 3.600e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(X_test[0])\n",
    "print(X[0])\n",
    "plt.imshow(np.reshape(X[0], [28, 28]), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mn.load_data()\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "f = open(\"shift2.txt\", 'w')\n",
    "(X_train, y_train), (X_test, y_test) = mn.load_data()\n",
    "X_test = X_test.astype('float32')\n",
    "n_images = 10000\n",
    "datagen = ImageDataGenerator()# fit parameters from data\n",
    "val = 0.0\n",
    "while val<4:\n",
    "    transform_parameters = { 'tx': val}\n",
    "    \n",
    "    X = datagen.apply_transform(X_test, transform_parameters)\n",
    "    X_test = X\n",
    "    x_te={'images': X_test}\n",
    "    input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x=x_te, shuffle=False)\n",
    "    # Use the model to predict the images class\n",
    "    preds = list(model.predict(input_fn))\n",
    "    accuracy = 0.0\n",
    "    # Display\n",
    "    for i in range(n_images):\n",
    "        if (preds[i] == y_test[i]):\n",
    "            accuracy = accuracy + 1/n_images\n",
    "#         plt.imshow(np.reshape(X_test[i], [28, 28]), cmap='gray')\n",
    "#         plt.show()\n",
    "#         print(\"Model prediction:\", preds[i])\n",
    "    print(\"Model accuracy:\", accuracy)\n",
    "    f.write(str(val)+\"\\t\"+str(accuracy)+\"\\n\")\n",
    "    val = val-0.2\n",
    "f.close()\n",
    "print(\"Transformation completed!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
